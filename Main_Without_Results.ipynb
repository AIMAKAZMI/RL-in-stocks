{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPT0ipYE28wL",
        "outputId": "64830031-c78c-4ffd-c22b-41ded31a42a9"
      },
      "outputs": [],
      "source": [
        "# NEED TO INSTALL THE FOLLOWING WITH PIP FIRST:\n",
        "\n",
        "\"\"\"\n",
        "wrds,\n",
        "swig,\n",
        "finrl==0.3.5\n",
        "elegantrl==0.3.3\n",
        "git+https://github.com/AI4Finance-Foundation/FinRL.git\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPqeTTwoh1hn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "from stable_baselines3.common.logger import configure\n",
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../FinRL\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtUc_ofKmpdy"
      },
      "outputs": [],
      "source": [
        "from finrl.main import check_and_make_directories\n",
        "from finrl.config import (\n",
        "    DATA_SAVE_DIR,\n",
        "    TRAINED_MODEL_DIR,\n",
        "    TENSORBOARD_LOG_DIR,\n",
        "    RESULTS_DIR,\n",
        ")\n",
        "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCKm4om-s9kE",
        "outputId": "e3e37681-442d-43e9-9c68-9b05074164a0"
      },
      "outputs": [],
      "source": [
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "\n",
        "tickers_list = ['MSFT', 'AAPL', 'CAT', 'CSCO', 'NKE']\n",
        "\n",
        "df = YahooDownloader(start_date = '2012-01-01',\n",
        "                     end_date = '2023-10-31',\n",
        "                     ticker_list = tickers_list).fetch_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzqRRTOX6aFu",
        "outputId": "736fd30d-93a5-4262-977b-d1674949af2c"
      },
      "outputs": [],
      "source": [
        "print(tickers_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CV3HrZHLh1hy",
        "outputId": "47d495fa-e186-4921-ec51-b01092859a52"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "4hYkeaPiICHS",
        "outputId": "c029d2f9-31f5-440e-83b6-5318c104210d"
      },
      "outputs": [],
      "source": [
        "# df.sort_values(['date','tic'],ignore_index=True).head()\n",
        "df.sort_values(['date'],ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmKP-1ii3RLS",
        "outputId": "b33407d9-0e79-485a-c292-2e329b2a2d88"
      },
      "outputs": [],
      "source": [
        "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "\n",
        "\n",
        "feature_engineer = FeatureEngineer(\n",
        "    use_technical_indicator=True,\n",
        "    tech_indicator_list = ['macd',\n",
        "    'boll_ub',\n",
        "    'boll_lb',\n",
        "    'rsi_30',\n",
        "    'cci_30',\n",
        "    'dx_30',\n",
        "    'close_30_sma',\n",
        "    'close_60_sma'],\n",
        "    use_vix=True,\n",
        "    use_turbulence=True,\n",
        "    user_defined_feature = False)\n",
        "\n",
        "states_df = feature_engineer.preprocess_data(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from itertools import product\n",
        "import pandas as pd\n",
        "dates = list(pd.date_range(states_df['date'].min(),states_df['date'].max()).astype(str))\n",
        "\n",
        "preprocessed_df = pd.DataFrame(list(product(dates,tickers_list)),columns=[\"date\",\"tic\"])\n",
        "preprocessed_df = preprocessed_df.merge(states_df,how=\"left\",on=[\"date\",\"tic\"],)\n",
        "preprocessed_df = preprocessed_df[preprocessed_df['date'].isin(states_df['date'])]\n",
        "preprocessed_df = preprocessed_df.sort_values(['date','tic'])\n",
        "\n",
        "preprocessed_df = preprocessed_df.fillna(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocessed_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0qaVGjLtgbI",
        "outputId": "0fc3244d-de6e-4519-992f-1f7e12a5596e"
      },
      "outputs": [],
      "source": [
        "train = data_split(preprocessed_df, '2012-01-01','2020-07-01')\n",
        "test = data_split(preprocessed_df, '2020-07-01','2023-10-31')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "p52zNCOhTtLR",
        "outputId": "e51da944-b23a-45d2-c60c-2457274dafa2"
      },
      "outputs": [],
      "source": [
        "train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2zqII8rMIqn",
        "outputId": "17d10eef-8f0b-4bce-a3fe-8a114f906e32"
      },
      "outputs": [],
      "source": [
        "stock_size = len(tickers_list)\n",
        "state_space = 1 + 2*stock_size + len(['macd',\n",
        "    'boll_ub',\n",
        "    'boll_lb',\n",
        "    'rsi_30',\n",
        "    'cci_30',\n",
        "    'dx_30',\n",
        "    'close_30_sma',\n",
        "    'close_60_sma'])*stock_size\n",
        "\n",
        "# 8 indictor/price features PER stock (there's 5 here)\n",
        "# Plus, the raw price data is captured again as 2 extra features per stock (typically Open and Close price).\n",
        "# So + 2N state variables\n",
        "# Hence 8*5 (indicators) + 2*5 (raw price) + 1 (offset variable)\n",
        "\n",
        "stock_size, state_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWyp84Ltto19"
      },
      "outputs": [],
      "source": [
        "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "\n",
        "\n",
        "training_environment, initial_observations = StockTradingEnv(df = train, hmax= 100,\n",
        "    initial_amount= 1000000, # STARTING AMOUNT HERE\n",
        "    num_stock_shares= [0] * stock_size,\n",
        "    buy_cost_pct= [.1/100] * stock_size, # Transaction fee percent of buys per stock\n",
        "    sell_cost_pct= [.1/100] * stock_size,\n",
        "    state_space= state_space,\n",
        "    stock_dim= stock_size, # Stock dimensions\n",
        "    tech_indicator_list= ['macd',\n",
        "    'boll_ub',\n",
        "    'boll_lb',\n",
        "    'rsi_30',\n",
        "    'cci_30',\n",
        "    'dx_30',\n",
        "    'close_30_sma',\n",
        "    'close_60_sma'],\n",
        "    action_space= stock_size, \n",
        "    reward_scaling= 1e-4).get_sb_env() \n",
        "# creates a vectorized environment compatible with Stable Baselines algorithms\n",
        "# uses DummyVecEnv from Stable Baselines to create a vectorized wrapper of the trading env\n",
        "# wraps the env in a Vectorized environment that handles all the multiprocessing - steps, resets etc.\n",
        "# It calls reset() on the vectorized env to get the initial observations\n",
        "\"\"\"\n",
        "The training process involves observing stock price change, taking an action and reward's calculation. By interacting with the market environment, the agent will eventually derive a trading strategy that may maximize (expected) rewards.\n",
        "\n",
        "Our market environment, based on OpenAI Gym, simulates stock markets with historical market data.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.plot import backtest_stats, get_baseline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUCnkn-HIbmj",
        "outputId": "baf0ae36-87e3-46c0-f243-dfb7a72bdda4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "# Automatically build models from list of model names\n",
        "agent = DRLAgent(training_environment)\n",
        "models=[]\n",
        "def log_model(model_name):\n",
        "    print(model_name)\n",
        "    model = agent.get_model(f\"{model_name}\")\n",
        "    model.set_logger(configure(RESULTS_DIR + f'/{model_name}', [\"stdout\", \"csv\", \"tensorboard\"]))\n",
        "    models.append((model_name, model))\n",
        "\n",
        "model_names=['a2c'\n",
        "            ,'ddpg'\n",
        "            # ,'ppo'\n",
        "            # ,'sac'\n",
        "            ]\n",
        "for model_name in model_names:\n",
        "    log_model(model_name)\n",
        "print(models)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a2c_params = {\n",
        "    \"n_steps\": 5,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 7e-4,\n",
        "    \"gamma\": 0.99,\n",
        "    \"gae_lambda\": 0.95\n",
        "}\n",
        "a2c_tuned_params = { # THESE PARAMS WERE OBTAINED AFTER OPTUNA\n",
        "    \"n_steps\": 1,\n",
        "    \"ent_coef\": 0.0755882482216129,\n",
        "    \"learning_rate\": 2.637065887731285e-05,\n",
        "    \"gamma\": 0.9048260592925886,\n",
        "    \"gae_lambda\": 0.9717236074963396\n",
        "}\n",
        "\n",
        "agent = DRLAgent(training_environment)\n",
        "a2c_model = agent.get_model(\"a2c\", model_kwargs=a2c_tuned_params)\n",
        "tmp_path = RESULTS_DIR + '/a2c'\n",
        "new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "a2c_model.set_logger(new_logger_a2c)\n",
        "trained_a2c = DRLAgent(training_environment).train_model(model=a2c_model,\n",
        "                             tb_log_name='a2c',\n",
        "                             total_timesteps=50000) \n",
        "\n",
        "\n",
        "# A2C HYPERPARAMS MOST IMPORTANT\n",
        "# learning_rate: The learning rate determines how quickly the model learns from new experiences. This is one of the most important to get right. Too small and it will learn slowly. Too large and it may have issues converging.\n",
        "# n_steps: The number of steps collected before each update. More steps allows more efficient batch updates but delays learning from recent experiences. Finding a good balance is important.\n",
        "# gamma: The discount factor determines how much the agent values future rewards. Higher values make it value long-term rewards more.\n",
        "# gae_lambda: The GAE lambda controls the bias-variance tradeoff for estimating returns. Values closer to 1 have lower variance but higher bias.\n",
        "# ent_coef: The entropy coefficient controls how much the agent is encouraged to explore randomly. Higher values result in more random actions.\n",
        "# max_grad_norm: Gradient clipping limit to improve stability. You generally don't need to tune this much.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "agent = DRLAgent(env = training_environment)\n",
        "ppo_params = {\n",
        "    \"n_steps\": 2048,\n",
        "    \"ent_coef\": 0.01,\n",
        "    \"learning_rate\": 0.00025,\n",
        "    \"batch_size\": 128,\n",
        "}\n",
        "\n",
        "ppo_tuned_params = {\n",
        "    \"n_steps\": 232,\n",
        "    \"ent_coef\": 0.08005421293955037,\n",
        "    \"learning_rate\": 0.0002058992300570136,\n",
        "    \"batch_size\": 238\n",
        "\n",
        "\n",
        "}\n",
        "model_ppo = agent.get_model(\"ppo\",model_kwargs = ppo_tuned_params)\n",
        "\n",
        "# set up logger\n",
        "new_logger_ppo = configure(RESULTS_DIR + '/ppo', [\"stdout\", \"csv\", \"tensorboard\"])\n",
        "# Set new logger\n",
        "model_ppo.set_logger(new_logger_ppo)\n",
        "\n",
        "trained_ppo = agent.train_model(model=model_ppo,\n",
        "                             tb_log_name='ppo',\n",
        "                             total_timesteps=50000)\n",
        "\n",
        "# DDPG HYPERPARAMS MOST IMPORTANT\n",
        "# learning_rate: Determines how quickly the model learns from new data. Too low may learn slowly, too high can destabilize training.\n",
        "# buffer_size: The size of the replay buffer holding experiences. Larger buffers allow longer term learning but cost memory.\n",
        "# batch_size: The size of sampled batch from the replay buffer for learning updates. Too small may underutilize GPU/CPU resources.\n",
        "# tau: Controls weighting between older and newer Q-network weights during update. Controls stability vs plasticity.\n",
        "# train_freq: How frequently the model trains. Balance between learning from more data vs more frequent updates.\n",
        "# gradient_steps: Number of gradient steps during each training update. More may increase stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gt8eIQKYM4G3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efwBi84ch1jE"
      },
      "outputs": [],
      "source": [
        "processed_risk_dfs = preprocessed_df[(preprocessed_df.date<'2020-07-01') & (preprocessed_df.date>='2012-01-01')]\n",
        "risk_df = processed_risk_dfs.drop_duplicates(subset=['date']) # INCLUDES THE VIX AND TURBULENCE INDICATORS ALONGSIDE WITH PREV TECHNICAL INDICATORS\n",
        "\n",
        "for col in risk_df: \n",
        "    print(col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHZMBpSqh1jG",
        "outputId": "bfc76e6c-2831-491e-efb3-4a0b28b5f7b8"
      },
      "outputs": [],
      "source": [
        "risk_df.vix.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDkszkMloRWT",
        "outputId": "40570340-db9c-4f27-e84c-c9998775c1c8"
      },
      "outputs": [],
      "source": [
        "risk_df.vix.quantile(0.996)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL7hs7svnNWT",
        "outputId": "cfed8ac7-406a-4ee6-e611-e83fb4f16de3"
      },
      "outputs": [],
      "source": [
        "risk_df.turbulence.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N78hfHckoqJ9",
        "outputId": "9912a51d-bcc2-4b99-bcfb-ecafa83f1446"
      },
      "outputs": [],
      "source": [
        "risk_df.turbulence.quantile(0.996)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIqoV0GSI52v"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# TESTING OVER HERE\n",
        "trading_environment = StockTradingEnv(df = test, turbulence_threshold = 70,risk_indicator_col='vix', hmax= 100,\n",
        "    initial_amount= 1000000, # STARTING AMOUNT HERE\n",
        "    num_stock_shares= [0] * stock_size,\n",
        "    buy_cost_pct= [.1/100] * stock_size, # Transaction fee percent of buys per stock\n",
        "    sell_cost_pct= [.1/100] * stock_size,\n",
        "    state_space= state_space,\n",
        "    stock_dim= stock_size, # Stock dimensions\n",
        "    tech_indicator_list= ['macd',\n",
        "    'boll_ub',\n",
        "    'boll_lb',\n",
        "    'rsi_30',\n",
        "    'cci_30',\n",
        "    'dx_30',\n",
        "    'close_30_sma',\n",
        "    'close_60_sma'],\n",
        "    action_space= stock_size, \n",
        "    reward_scaling= 1e-4)\n",
        "env_trade, obs_trade = trading_environment.get_sb_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLOnL5eYh1jR",
        "outputId": "fd6f7bb2-8fae-4fa9-cf06-e0167a6920ee"
      },
      "outputs": [],
      "source": [
        "# trained_model_ddpg = trained_ddpg\n",
        "# df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
        "#     model=trained_model_ddpg,\n",
        "#     environment = trading_environment)\n",
        "\n",
        "\n",
        "testing_a2c_model = trained_a2c\n",
        "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
        "    model=testing_a2c_model,\n",
        "    environment = trading_environment)\n",
        "\n",
        "testing_ppo_model = trained_ppo\n",
        "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
        "    model=testing_ppo_model,\n",
        "    environment = trading_environment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train\n",
        "a2c_params =  testing_a2c_model.get_parameters()\n",
        "a2c_params['policy']\n",
        "a2c_params['policy.optimizer']    \n",
        "df_account_value_a2c\n",
        "df_actions_a2c\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "a2c_performance_stats = pd.DataFrame(backtest_stats(account_value=df_account_value_a2c))\n",
        "# a2c_performance_stats.to_csv(\"./\"+RESULTS_DIR+\"/a2c_performance_stats\"+datetime.datetime.now().strftime('%Y%m%d-%Hh%M')+'.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppo_performance_stats = pd.DataFrame(backtest_stats(account_value=df_account_value_ppo))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cumulative_return_value = a2c_performance_stats.loc[\"Cumulative returns\"].iloc[0]\n",
        "print(\"Cumulative Returns Value:\", cumulative_return_value)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkV-LB66iwhD",
        "outputId": "3deaecb7-7307-4b42-ed26-14511d0ddc41"
      },
      "outputs": [],
      "source": [
        "baseline_df = get_baseline(\n",
        "        ticker=\"^NDX\",\n",
        "        start = df_account_value_a2c.loc[0,'date'],\n",
        "        end = df_account_value_a2c.loc[len(df_account_value_a2c)-1,'date'])\n",
        "\n",
        "stats = backtest_stats(baseline_df, value_col_name = 'close')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_df = get_baseline(\n",
        "        ticker=\"^NDX\",\n",
        "        start = df_account_value_ppo.loc[0,'date'],\n",
        "        end = df_account_value_ppo.loc[len(df_account_value_ppo)-1,'date'])\n",
        "\n",
        "stats = backtest_stats(baseline_df, value_col_name = 'close')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_account_value_a2c['date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qg1kvfemrrQH",
        "outputId": "94f27dce-2615-4401-b12e-2386cc9a408b"
      },
      "outputs": [],
      "source": [
        "df_account_value_a2c.loc[0,'date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tt1bzL5OrsTa",
        "outputId": "3abd2d76-36d9-47ad-898d-5a2d5d8a30a1"
      },
      "outputs": [],
      "source": [
        "df_account_value_a2c.loc[len(df_account_value_a2c)-1,'date']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "tune_hyper_params =False\n",
        "\n",
        "# BE CAREUFL THS CAN TAKE HOURS. RUN ONLY IF NEEDED\n",
        "\n",
        "def evaluate_model(model, trading_environment):\n",
        "    df_account_value, _ = DRLAgent.DRL_prediction(model=model, environment=trading_environment)\n",
        "    perf_stats = pd.DataFrame(backtest_stats(account_value=df_account_value))\n",
        "    return perf_stats\n",
        "\n",
        "n_steps_values = [5, 10, 15]\n",
        "ent_coef_values = [0.01, 0.1, 0.2]\n",
        "learning_rate_values = [1e-4, 5e-4, 1e-3]\n",
        "\n",
        "best_performance = float('-inf')\n",
        "best_params = None\n",
        "if tune_hyper_params:\n",
        "    # Training and evaluation loop\n",
        "    for n_steps in n_steps_values:\n",
        "        for ent_coef in ent_coef_values:\n",
        "            for learning_rate in learning_rate_values:\n",
        "                a2c_params = {\n",
        "                    \"n_steps\": n_steps,\n",
        "                    \"ent_coef\": ent_coef,\n",
        "                    \"learning_rate\": learning_rate,\n",
        "                    \"gamma\": 0.99,\n",
        "                    \"gae_lambda\": 0.95\n",
        "                }\n",
        "\n",
        "                agent = DRLAgent(training_environment)\n",
        "                a2c_model = agent.get_model(\"a2c\", model_kwargs=a2c_params)\n",
        "                trained_a2c = DRLAgent(training_environment).train_model(model=a2c_model, tb_log_name='a2c', total_timesteps=50000)\n",
        "\n",
        "                perf_stats = evaluate_model(trained_a2c, trading_environment)\n",
        "\n",
        "                if perf_stats.loc[\"Cumulative returns\"].iloc[-1] > best_performance:\n",
        "                    best_performance = perf_stats.loc[\"Cumulative returns\"].iloc[-1]\n",
        "                    best_params = a2c_params\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Performance (Cumulative Return):\", best_performance)\n",
        "\n",
        "#TOOK 57 MINS:\n",
        "# Best Hyperparameters: {'n_steps': 15, 'ent_coef': 0.2, 'learning_rate': 0.0005, 'gamma': 0.99, 'gae_lambda': 0.95}\n",
        "# Best Performance (Cumulative Return): 0.822119795194425\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna import Trial\n",
        "\n",
        "def objective(trial: Trial, model_name=None):\n",
        "    # Define the search space for hyperparameters\n",
        "    a2c_params = {\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 1, 10),\n",
        "        \"ent_coef\": trial.suggest_float(\"ent_coef\", 0.001, 0.1),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.9, 0.999),\n",
        "        \"gae_lambda\": trial.suggest_float(\"gae_lambda\", 0.9, 0.999)\n",
        "    }\n",
        "\n",
        "    # Train the A2C model with the current set of hyperparameters\n",
        "    agent = DRLAgent(training_environment)\n",
        "    a2c_model = agent.get_model(\"a2c\", model_kwargs=a2c_params)\n",
        "    trained_a2c = DRLAgent(training_environment).train_model(\n",
        "        model=a2c_model,\n",
        "        tb_log_name='a2c',\n",
        "        total_timesteps=50000\n",
        "    ) \n",
        "\n",
        "    # Evaluate the model\n",
        "    trading_environment = StockTradingEnv(df=test, turbulence_threshold=70, risk_indicator_col='vix', hmax=100,\n",
        "                                          initial_amount=1000000, num_stock_shares=[0] * stock_size,\n",
        "                                          buy_cost_pct=[.1/100] * stock_size, sell_cost_pct=[.1/100] * stock_size,\n",
        "                                          state_space=state_space, stock_dim=stock_size,\n",
        "                                          tech_indicator_list=['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30',\n",
        "                                                               'close_30_sma', 'close_60_sma'],\n",
        "                                          action_space=stock_size, reward_scaling=1e-4)\n",
        "    # env_trade, obs_trade = trading_environment.get_sb_env()\n",
        "\n",
        "    # df_account_value_a2c, _ = DRLAgent.DRL_prediction(model=trained_a2c, environment=trading_environment)\n",
        "    perf_stats = evaluate_model(trained_a2c, trading_environment)\n",
        "\n",
        "    # Return the metric to be optimized (negative because Optuna minimizes)\n",
        "    return -perf_stats.loc[\"Cumulative returns\"].iloc[-1]\n",
        "\n",
        "if tune_hyper_params:\n",
        "    # Create a study object and optimize the objective function\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=50)\n",
        "\n",
        "    # Print the best parameters found by Optuna\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"Value: \", trial.value)\n",
        "    print(\"Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n",
        "    \"\"\" \n",
        "    Best trial:\n",
        "Value:  0.12409658470314999\n",
        "Params: \n",
        "    n_steps: 1\n",
        "    ent_coef: 0.0755882482216129\n",
        "    learning_rate: 2.637065887731285e-05\n",
        "    gamma: 0.9048260592925886\n",
        "    gae_lambda: 0.9717236074963396\n",
        "\n",
        "    \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna.visualization as optuna_viz\n",
        "\n",
        "if tune_hyper_params:\n",
        "# Plot parameter importances\n",
        "    optuna_viz.plot_param_importances(study)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if tune_hyper_params:\n",
        "    optuna_viz.plot_optimization_history(study)\n",
        "# This plot tells us that Optuna made the score converge to the minimum after only a few trials.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from stable_baselines3.common.noise import NormalActionNoise\n",
        "\n",
        "from optuna import Trial\n",
        "\n",
        "def objective(trial: Trial):\n",
        "    # Define the search space for hyperparameters\n",
        "    ppo_params = {\n",
        "        \"n_steps\": trial.suggest_int(\"n_steps\", 16, 512),\n",
        "        \"ent_coef\": trial.suggest_float(\"ent_coef\", 0.01, 0.1),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3),\n",
        "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 256),\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    # Train the PPO model with the current set of hyperparameters\n",
        "    agent = DRLAgent(training_environment)\n",
        "    ppo_model = agent.get_model(\"ppo\", model_kwargs=ppo_params)\n",
        "    trained_ppo = DRLAgent(training_environment).train_model(\n",
        "        model=ppo_model,\n",
        "        tb_log_name='ppo',\n",
        "        total_timesteps=50000\n",
        "    ) \n",
        "\n",
        "    # Evaluate the model\n",
        "    trading_environment = StockTradingEnv(df=test, turbulence_threshold=70, risk_indicator_col='vix', hmax=100,\n",
        "                                          initial_amount=1000000, num_stock_shares=[0] * stock_size,\n",
        "                                          buy_cost_pct=[.1/100] * stock_size, sell_cost_pct=[.1/100] * stock_size,\n",
        "                                          state_space=state_space, stock_dim=stock_size,\n",
        "                                          tech_indicator_list=['macd', 'boll_ub', 'boll_lb', 'rsi_30', 'cci_30', 'dx_30',\n",
        "                                                               'close_30_sma', 'close_60_sma'],\n",
        "                                          action_space=stock_size, reward_scaling=1e-4)\n",
        "    # env_trade, obs_trade = trading_environment.get_sb_env()\n",
        "\n",
        "    perf_stats = evaluate_model(trained_ppo, trading_environment)\n",
        "\n",
        "    # Return the metric to be optimized (negative because Optuna minimizes)\n",
        "    return -perf_stats.loc[\"Cumulative returns\"].iloc[-1]\n",
        "if tune_hyper_params:\n",
        "    # Create a study object and optimize the objective function\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=50)\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "    print(\"Value: \", trial.value)\n",
        "    print(\"Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(f\"    {key}: {value}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if tune_hyper_params:\n",
        "    optuna_viz.plot_param_importances(study)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if tune_hyper_params:\n",
        "    optuna_viz.plot_optimization_history(study)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_account_value_ppo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# S&P 500: ^GSPC\n",
        "# Dow Jones Index: ^DJI\n",
        "# NASDAQ 100: ^NDX\n",
        "df_ndx_ = get_baseline(\n",
        "        ticker=\"^NDX\", \n",
        "        start = '2020-07-01',\n",
        "        end = '2023-10-31')\n",
        "stats = backtest_stats(df_ndx_, value_col_name = 'close')\n",
        "df_ndx = pd.DataFrame()\n",
        "df_ndx['date'] = df_account_value_a2c['date']\n",
        "df_ndx['account_value'] = df_ndx_['close'] / df_ndx_['close'][0] * 1000000 # INITIAL AMOUNT HERE!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_ndx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CAN DO BEFORE TUNING AND AFTER TUNING PARAMS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Plotting code\n",
        "\n",
        "plt.figure(figsize=(20, 12))\n",
        "plt.plot(df_account_value_a2c.index, df_account_value_a2c['account_value'], label='A2C')\n",
        "# plt.plot(df_account_value_ddpg.index, df_account_value_ddpg['account_value'], label='DDPG')\n",
        "plt.plot(df_account_value_ppo.index, df_account_value_ppo['account_value'], label='PPO')\n",
        "plt.plot(df_ndx.index, df_ndx['account_value'], label='NDX')\n",
        "\n",
        "plt.title('A2C vs PPO Performance')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Account Value')\n",
        "plt.legend()\n",
        "\n",
        "# Add grid lines\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_gDkU-j-fCmZ",
        "3Zpv4S0-fDBv"
      ],
      "name": "Stock_NeurIPS2018.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
